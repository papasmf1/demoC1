import requests
from bs4 import BeautifulSoup
import time
import json
from urllib.parse import urljoin, urlparse, parse_qs
import re

class NaverNewsCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def is_valid_news_url(self, url):
        """ìœ íš¨í•œ ë‰´ìŠ¤ URLì¸ì§€ í™•ì¸"""
        if not url:
            return False
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ URL íŒ¨í„´ í™•ì¸
        valid_patterns = [
            'n.news.naver.com/mnews/article',
            'news.naver.com/main/read',
            'news.naver.com/article'
        ]
        
        # ì œì™¸í•  URL íŒ¨í„´
        exclude_patterns = [
            'news.naver.com/main/list',
            'news.naver.com/main/ranking',
            'news.naver.com/main/hotissue',
            'media.naver.com',
            '#'
        ]
        
        # ì œì™¸ íŒ¨í„´ ê²€ì‚¬
        for pattern in exclude_patterns:
            if pattern in url:
                return False
        
        # ìœ íš¨ íŒ¨í„´ ê²€ì‚¬
        for pattern in valid_patterns:
            if pattern in url:
                return True
        
        return False
        
    def get_search_results(self, search_url):
        """ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë§í¬ë“¤ì„ ì¶”ì¶œ"""
        try:
            response = self.session.get(search_url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‰´ìŠ¤ ì„¹ì…˜ ì°¾ê¸°
            news_links = []
            news_data = []
            
            # ì‹¤ì œ ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡°ì— ë§ëŠ” ì…€ë ‰í„°ë“¤
            selectors = [
                # ë©”ì¸ ë‰´ìŠ¤ ì œëª© ë§í¬
                'a.UpDjg8Q2DzdaIi4sfrjX',
                # ì„œë¸Œ ë‰´ìŠ¤ ì œëª© ë§í¬  
                'a.Oxs1v7upclSMaxokltKZ',
                # ë„¤ì´ë²„ë‰´ìŠ¤ ë§í¬
                'a[href*="n.news.naver.com"]',
                # ì¼ë°˜ ë‰´ìŠ¤ ë§í¬
                'a[href*="news.naver.com"]'
            ]
            
            # ë‰´ìŠ¤ ì•„ì´í…œë“¤ ì°¾ê¸°
            news_items = soup.select('.fds-news-item-list-desk .NYqAjUWdQsgkJBAODPln')
            
            for item in news_items:
                # ë©”ì¸ ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬
                main_title_link = item.select_one('a.UpDjg8Q2DzdaIi4sfrjX')
                if main_title_link:
                    title = main_title_link.get_text().strip()
                    href = main_title_link.get('href')
                    
                    # ì–¸ë¡ ì‚¬ ì •ë³´
                    press = item.select_one('.sds-comps-profile-info-title-text')
                    press_name = press.get_text().strip() if press else "Unknown"
                    
                    # ì‹œê°„ ì •ë³´
                    time_elem = item.select_one('.RhtLWxQlRdnXvHdGqikm span')
                    time_info = time_elem.get_text().strip() if time_elem else "Unknown"
                    
                    # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°
                    preview = item.select_one('a.qayQSl_GP1qS0BX8dYlm span')
                    preview_text = preview.get_text().strip() if preview else ""
                    
                    if href and self.is_valid_news_url(href):
                        news_data.append({
                            'url': href,
                            'title': title,
                            'press': press_name,
                            'time': time_info,
                            'preview': preview_text
                        })
                        news_links.append(href)
                
                # ì„œë¸Œ ë‰´ìŠ¤ë“¤ë„ ì¶”ê°€
                sub_links = item.select('a.Oxs1v7upclSMaxokltKZ')
                for sub_link in sub_links:
                    href = sub_link.get('href')
                    title = sub_link.get_text().strip()
                    if href and title and self.is_valid_news_url(href):
                        news_data.append({
                            'url': href,
                            'title': title,
                            'press': "Sub News",
                            'time': "Unknown",
                            'preview': ""
                        })
                        news_links.append(href)
            
            # ì¶”ê°€ ê²€ìƒ‰: ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ë§í¬ë“¤
            for selector in selectors:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href')
                    if href and self.is_valid_news_url(href) and href not in news_links:
                        news_links.append(href)
            
            # ì¤‘ë³µ ì œê±°
            news_links = list(dict.fromkeys(news_links))  # ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
            
            print(f"ë°œê²¬ëœ ë‰´ìŠ¤ ë§í¬ ìˆ˜: {len(news_links)}")
            print(f"ë‰´ìŠ¤ ë°ì´í„° ìˆ˜: {len(news_data)}")
            
            # ë””ë²„ê¹…: ì²« ëª‡ ê°œ ë‰´ìŠ¤ ì •ë³´ ì¶œë ¥
            for i, data in enumerate(news_data[:3]):
                print(f"ë‰´ìŠ¤ {i+1}: {data['title'][:50]}... ({data['press']})")
            
            return news_links
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def extract_news_content(self, news_url):
        """ê°œë³„ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œ"""
        try:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ í˜ì´ì§€ ì ‘ê·¼
            response = self.session.get(news_url)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì œëª© ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ ìµœì‹  êµ¬ì¡° ë°˜ì˜)
            title_selectors = [
                '#title_area h2',  # ìµœì‹  ë„¤ì´ë²„ë‰´ìŠ¤
                '.media_end_head_headline h2',  # ì¼ë°˜ì ì¸ ë„¤ì´ë²„ë‰´ìŠ¤
                '#articleTitle',  # êµ¬ë²„ì „
                '.newsct_article h2',  # ë‹¤ë¥¸ ë²„ì „
                'h1.title',  # ì¼ë¶€ ì–¸ë¡ ì‚¬
                '.article_title',  # ë°±ì—…
                'h1',  # ìµœí›„ ìˆ˜ë‹¨
                'h2'   # ìµœí›„ ìˆ˜ë‹¨
            ]
            
            title = None
            for selector in title_selectors:
                title_element = soup.select_one(selector)
                if title_element:
                    title = title_element.get_text().strip()
                    # HTML íƒœê·¸ê°€ ë‚¨ì•„ìˆë‹¤ë©´ ì œê±°
                    title = re.sub(r'<[^>]+>', '', title)
                    if title and len(title) > 5:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                        break
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ ìµœì‹  êµ¬ì¡° ë°˜ì˜)
            content_selectors = [
                '#dic_area',  # ìµœì‹  ë„¤ì´ë²„ë‰´ìŠ¤ ë³¸ë¬¸
                '#articleBodyContents',  # ì¼ë°˜ì ì¸ ë„¤ì´ë²„ë‰´ìŠ¤
                '.newsct_article ._article_body_contents',  # ë‹¤ë¥¸ ë²„ì „
                '._article_body_contents',  # êµ¬ë²„ì „
                '.se_component_wrap',  # ë„¤ì´ë²„ í¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
                '.article_body',  # ì¼ë¶€ ì–¸ë¡ ì‚¬
                '#content',  # ì¼ë°˜ì ì¸ content
                '.content',  # ë°±ì—…
                'article',  # HTML5 semantic
                '.article_txt'  # ì¶”ê°€ ë°±ì—…
            ]
            
            content = None
            for selector in content_selectors:
                content_element = soup.select_one(selector)
                if content_element:
                    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                    for unwanted in content_element(['script', 'style', 'aside', 'footer', 'nav', '.ad', '.advertisement']):
                        unwanted.decompose()
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    content = content_element.get_text()
                    
                    # í…ìŠ¤íŠ¸ ì •ë¦¬
                    content = re.sub(r'\s+', ' ', content)  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
                    content = re.sub(r'\n+', '\n', content)  # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ
                    content = content.strip()
                    
                    # ë„ˆë¬´ ì§§ì€ ë‚´ìš© ì œì™¸
                    if content and len(content) > 100:
                        break
                    else:
                        content = None
            
            # ë‚ ì§œ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ ìµœì‹  êµ¬ì¡° ë°˜ì˜)
            date_selectors = [
                '.media_end_head_info_datestamp_time',  # ìµœì‹ 
                '.media_end_head_info_datestamp time',
                '._ARTICLE_DATE_TIME',  # êµ¬ë²„ì „
                '.article_date',
                '.date',
                'time',
                '.published'
            ]
            
            date = None
            for selector in date_selectors:
                date_element = soup.select_one(selector)
                if date_element:
                    # datetime ì†ì„±ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                    date = date_element.get('datetime') or date_element.get_text().strip()
                    if date:
                        break
            
            # ì–¸ë¡ ì‚¬ ì •ë³´ ì¶”ì¶œ
            press_selectors = [
                '.media_end_head_top_logo img',
                '.press_logo img',
                '.logo img'
            ]
            
            press = None
            for selector in press_selectors:
                press_element = soup.select_one(selector)
                if press_element:
                    press = press_element.get('alt', '').replace('ì˜ í”„ë¡œí•„ ì´ë¯¸ì§€', '').strip()
                    if press:
                        break
            
            # ìš”ì•½ë¬¸ ì¶”ì¶œ ì‹œë„
            summary_selectors = [
                '.media_end_head_summary',
                '.article_summary',
                '.summary'
            ]
            
            summary = None
            for selector in summary_selectors:
                summary_element = soup.select_one(selector)
                if summary_element:
                    summary = summary_element.get_text().strip()
                    if summary:
                        break
            
            if title and content:
                result = {
                    'url': news_url,
                    'title': title,
                    'content': content,
                    'date': date,
                    'press': press,
                    'summary': summary
                }
                return result
            else:
                print(f"ì œëª© ë˜ëŠ” ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {news_url}")
                print(f"  ì œëª©: {'ì°¾ìŒ' if title else 'ëª»ì°¾ìŒ'}")
                print(f"  ë³¸ë¬¸: {'ì°¾ìŒ' if content else 'ëª»ì°¾ìŒ'}")
                
                # ë””ë²„ê¹…: í˜ì´ì§€ì˜ ì£¼ìš” íƒœê·¸ í™•ì¸
                print("  í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ì£¼ìš” íƒœê·¸ë“¤:")
                for tag in ['h1', 'h2', 'h3', 'article', '#content', '.content']:
                    elements = soup.select(tag)
                    if elements:
                        print(f"    {tag}: {len(elements)}ê°œ")
                
                return None
                
        except Exception as e:
            print(f"ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ({news_url}): {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def crawl_news(self, search_url, max_articles=10):
        """ë©”ì¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
        print("ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        
        # 1. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‰´ìŠ¤ ë§í¬ ì¶”ì¶œ
        news_links = self.get_search_results(search_url)
        
        if not news_links:
            print("ë‰´ìŠ¤ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 2. ê° ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§
        articles = []
        processed = 0
        
        for link in news_links[:max_articles]:
            if processed >= max_articles:
                break
                
            print(f"í¬ë¡¤ë§ ì¤‘... ({processed + 1}/{min(len(news_links), max_articles)})")
            
            article = self.extract_news_content(link)
            if article:
                articles.append(article)
                processed += 1
            
            # ìš”ì²­ ê°„ê²© ì¡°ì ˆ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
            time.sleep(1)
        
        print(f"í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        return articles
    
    def save_to_json(self, articles, filename='news_articles.json'):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        print(f"ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def print_articles(self, articles):
        """ê¸°ì‚¬ë“¤ì„ ì½˜ì†”ì— ì¶œë ¥"""
        for i, article in enumerate(articles, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“° ê¸°ì‚¬ {i}")
            print(f"{'='*60}")
            print(f"ğŸ“Œ ì œëª©: {article['title']}")
            print(f"ğŸ¢ ì–¸ë¡ ì‚¬: {article.get('press', 'N/A')}")
            print(f"ğŸ“… ë‚ ì§œ: {article.get('date', 'N/A')}")
            print(f"ğŸ”— URL: {article['url']}")
            
            # ìš”ì•½ë¬¸ì´ ìˆìœ¼ë©´ ì¶œë ¥
            if article.get('summary'):
                print(f"ğŸ“ ìš”ì•½: {article['summary'][:150]}...")
            
            # ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°
            content_preview = article['content'][:300] if len(article['content']) > 300 else article['content']
            print(f"ğŸ“„ ë³¸ë¬¸: {content_preview}...")
            print(f"   (ì „ì²´ ê¸¸ì´: {len(article['content'])}ì)")

def main():
    print("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 50)
    
    # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = NaverNewsCrawler()
    
    # ì£¼ì–´ì§„ URL
    search_url = "https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%B0%98%EB%8F%84%EC%B2%B4&ackey=guszjn2h"
    
    print(f"ğŸ” ê²€ìƒ‰ URL: {search_url}")
    print(f"ğŸ“ˆ í‚¤ì›Œë“œ: ë°˜ë„ì²´")
    print("-" * 50)
    
    # ë‰´ìŠ¤ í¬ë¡¤ë§ (ìµœëŒ€ 15ê°œ ê¸°ì‚¬)
    articles = crawler.crawl_news(search_url, max_articles=15)
    
    if articles:
        print(f"\nâœ… ì´ {len(articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤!")
        
        # ê²°ê³¼ ì¶œë ¥
        crawler.print_articles(articles)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        crawler.save_to_json(articles, 'semiconductor_news.json')
        
        print(f"\nğŸ’¾ ëª¨ë“  ê¸°ì‚¬ê°€ 'semiconductor_news.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        
    else:
        print("âŒ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
        print("   - ì¸í„°ë„· ì—°ê²° ìƒíƒœ")
        print("   - ë„¤ì´ë²„ ì ‘ê·¼ ê°€ëŠ¥ ì—¬ë¶€")
        print("   - URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")

def demo():
    """ë°ëª¨ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¥ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ë°ëª¨")
    print("ì´ ë°ëª¨ëŠ” ë°˜ë„ì²´ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
    
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()